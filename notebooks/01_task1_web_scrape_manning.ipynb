{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5be6e39",
   "metadata": {},
   "source": [
    "# CW1 — Task 1: Web Scraping (Manning) → CSV\n",
    "\n",
    "**Name:** Akia Hans Swin Carreon\n",
    "**Student ID UoR:** *carreona@roehampton.ac.uk*\n",
    "**Student ID Lithan:** *LS07432@learning.lithan.com*\n",
    "\n",
    "Goal: Scrape at least **15** Data Engineering book records from Manning and save to CSV.\n",
    "Output: `data/processed/books.csv`\n",
    "\n",
    "This notebook runs the same logic as:\n",
    "\n",
    "- `python -m src.task1_scrape.manning_run`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bbf1f",
   "metadata": {},
   "source": [
    "## Install Dependencies (If needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7afda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your venv already has these, you can skip this cell.\n",
    "# %pip works in VS Code notebooks.\n",
    "%pip install -q requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af41c3",
   "metadata": {},
   "source": [
    "## Imports, Config, Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Output folders\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Catalog URLs (add/remove if needed)\n",
    "CATALOG_URLS = [\n",
    "    \"https://www.manning.com/catalog/software-development/cloud/data-engineering\",\n",
    "    \"https://www.manning.com/catalog/software-development/cloud/data-engineering/cloud-data-platforms\",\n",
    "    \"https://www.manning.com/catalog/software-development/cloud/data-engineering/big-data-processing\",\n",
    "    \"https://www.manning.com/catalog/software-development/cloud/data-engineering/cloud-data-engineering\",\n",
    "    \"https://www.manning.com/catalog/software-development/databases/database-platforms/azure-data-engineering\",\n",
    "    \"https://www.manning.com/catalog/data-science/data-engineering/data-management-and-organization\",\n",
    "]\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Referer\": \"https://www.manning.com/catalog\",\n",
    "}\n",
    "\n",
    "YEAR_RE = re.compile(r\"^(19|20)\\d{2}$\")\n",
    "PRICE_LINE_RE = re.compile(r\"^([$£€])\\s*([0-9]+(?:\\.[0-9]{2})?)$\")\n",
    "RATINGCOUNT_RE = re.compile(r\"\\(\\s*(\\d+)\\s*\\)\")  # more tolerant than strict match\n",
    "\n",
    "NOISE = {\n",
    "    \"manning.com\", \"/\", \"catalog\", \"browse\", \"home\", \"cart\", \"log in\",\n",
    "    \"sort:\", \"newest\", \"popularity\",\n",
    "    \"Software Development\", \"Cloud\", \"Data Engineering\",\n",
    "    \"Databases\", \"Database Platforms\",\n",
    "}\n",
    "NOISE_LOWER = {s.lower() for s in NOISE}\n",
    "\n",
    "DEBUG = False  # set True for per-row debug\n",
    "\n",
    "\n",
    "def fetch_html(url: str, timeout: int = 30) -> str:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "    print(f\"[FETCH] {r.status_code} {url}\")\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def clean_lines(html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    lines = [ln.strip() for ln in soup.get_text(\"\\n\", strip=True).split(\"\\n\")]\n",
    "    return [ln for ln in lines if ln]\n",
    "\n",
    "\n",
    "def is_noise(line: str) -> bool:\n",
    "    low = line.strip().lower()\n",
    "    if low in NOISE_LOWER:\n",
    "        return True\n",
    "    if line.strip() in {\",\", \"|\"}:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_year(line: str) -> bool:\n",
    "    return YEAR_RE.match(line.strip()) is not None\n",
    "\n",
    "\n",
    "def is_price(line: str) -> bool:\n",
    "    return PRICE_LINE_RE.match(line.strip()) is not None\n",
    "\n",
    "\n",
    "def price_value(line: str) -> Optional[float]:\n",
    "    m = PRICE_LINE_RE.match(line.strip())\n",
    "    if not m:\n",
    "        return None\n",
    "    return float(m.group(2))\n",
    "\n",
    "\n",
    "def looks_like_title(line: str) -> bool:\n",
    "    s = line.strip()\n",
    "    if not s or len(s) < 6:\n",
    "        return False\n",
    "    if is_noise(s):\n",
    "        return False\n",
    "    if is_year(s) or is_price(s) or RATINGCOUNT_RE.search(s):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def dbg_row(title, authors, year, prices, chosen_price, star_rating, url, year_idx, idx_after, lines):\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"[ROW DEBUG]\")\n",
    "    print(f\"Title      : {title!r}\")\n",
    "    print(f\"Authors    : {authors!r}\")\n",
    "    print(f\"Year       : {year!r}\")\n",
    "    print(f\"Prices(raw): {prices!r} -> chosen: {chosen_price!r}\")\n",
    "    print(f\"Rating(cnt): {star_rating!r}\")\n",
    "    print(f\"Source URL : {url}\")\n",
    "    print(f\"year_idx   : {year_idx} | idx(after scan): {idx_after}\")\n",
    "    print(\"-\" * 72)\n",
    "    start = max(year_idx - 4, 0)\n",
    "    end = min(year_idx + 12, len(lines))\n",
    "    for w in lines[start:end]:\n",
    "        print(\"  \", w)\n",
    "    print(\"=\" * 72)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302b133",
   "metadata": {},
   "source": [
    "## Parser & Scraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_catalog(html: str, catalog_url: str) -> list[dict[str, Any]]:\n",
    "    lines = clean_lines(html)\n",
    "    n = len(lines)\n",
    "\n",
    "    rows = []\n",
    "    seen = set()\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        if not looks_like_title(lines[i]):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        title = lines[i].strip()\n",
    "\n",
    "        # locate year within next 10 lines\n",
    "        year_idx = None\n",
    "        for j in range(i + 1, min(i + 10, n)):\n",
    "            if is_year(lines[j]):\n",
    "                year_idx = j\n",
    "                break\n",
    "        if year_idx is None:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # authors between title and year\n",
    "        author_parts = []\n",
    "        for k in range(i + 1, year_idx):\n",
    "            part = lines[k].strip()\n",
    "            if part in {\",\", \"|\"}:\n",
    "                continue\n",
    "            if is_noise(part) or is_price(part) or is_year(part) or RATINGCOUNT_RE.search(part):\n",
    "                continue\n",
    "            author_parts.append(part)\n",
    "\n",
    "        authors = \" \".join(author_parts).strip() if author_parts else None\n",
    "        year = int(lines[year_idx].strip())\n",
    "\n",
    "        # scan after year for prices + rating\n",
    "        scan_start = year_idx + 1\n",
    "        scan_end = min(year_idx + 15, n)\n",
    "\n",
    "        prices = []\n",
    "        idx = scan_start\n",
    "\n",
    "        # collect consecutive price lines\n",
    "        while idx < scan_end:\n",
    "            p = price_value(lines[idx])\n",
    "            if p is not None:\n",
    "                prices.append(p)\n",
    "                idx += 1\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        # rating count anywhere near year/price region\n",
    "        star_rating = None\n",
    "        for r_i in range(scan_start, scan_end):\n",
    "            m = RATINGCOUNT_RE.search(lines[r_i])\n",
    "            if m:\n",
    "                star_rating = float(m.group(1))\n",
    "                break\n",
    "\n",
    "        if not prices or not authors:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        price = prices[-1]  # pick the last displayed (often discounted)\n",
    "        if title in seen:\n",
    "            i = max(i + 1, idx)\n",
    "            continue\n",
    "        seen.add(title)\n",
    "\n",
    "        row = {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"star_rating\": star_rating,  # count shown in parentheses if present\n",
    "            \"price\": price,\n",
    "            \"source_url\": catalog_url,\n",
    "        }\n",
    "\n",
    "        if DEBUG:\n",
    "            dbg_row(title, authors, year, prices, price, star_rating, catalog_url, year_idx, idx, lines)\n",
    "\n",
    "        rows.append(row)\n",
    "        i = max(i + 1, idx)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def scrape_manning(catalog_urls: list[str]) -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    for idx, url in enumerate(catalog_urls, start=1):\n",
    "        print(f\"\\n=== CATALOG {idx}/{len(catalog_urls)} ===\")\n",
    "        html = fetch_html(url)\n",
    "        rows = parse_catalog(html, url)\n",
    "        print(f\"[PARSE] {len(rows)} rows from catalog\")\n",
    "        all_rows.extend(rows)\n",
    "        time.sleep(1)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No rows scraped. Add more catalog URLs or adjust parser.\")\n",
    "\n",
    "    df = (\n",
    "        df.dropna(subset=[\"title\", \"authors\", \"year\", \"price\"])\n",
    "          .drop_duplicates(subset=[\"title\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e3dfe",
   "metadata": {},
   "source": [
    "## Run & Save (15) Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_manning(CATALOG_URLS)\n",
    "\n",
    "df_15 = df.head(15).copy()\n",
    "print(\"[DF] shape:\", df_15.shape)\n",
    "display(df_15)\n",
    "\n",
    "raw_path = RAW_DIR / \"books_manning_raw.csv\"\n",
    "processed_path = PROCESSED_DIR / \"books.csv\"\n",
    "\n",
    "df_15.to_csv(raw_path, index=False)\n",
    "df_15.to_csv(processed_path, index=False)\n",
    "\n",
    "print(\"Saved RAW:\", raw_path)\n",
    "print(\"Saved PROCESSED:\", processed_path)\n",
    "print(\"Non-null star_rating:\", df_15[\"star_rating\"].notna().sum(), \"of\", len(df_15))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
